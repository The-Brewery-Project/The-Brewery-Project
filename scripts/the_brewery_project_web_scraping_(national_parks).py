# -*- coding: utf-8 -*-
"""The Brewery Project Web Scraping (National Parks).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CQal7wCpJqjpoAsie7vzPFFtIKd3vfMu

**Web Scraping**
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
from IPython.display import FileLink
from IPython.display import display
import re

# Build scraper

url = "https://en.wikipedia.org/wiki/List_of_national_parks_of_the_United_States"

response = requests.get(url)

soup = BeautifulSoup(response.text, 'html.parser')

state_headings = soup.find_all("h3")

national_parks_data = []

table = soup.find("table", {"class": "wikitable"})

for row in table.find_all("tr")[1:]:
  columns = row.find_all("td")
  if len(columns) >= 7:
    park_link = columns[0].find("a")
    if park_link:
      park_name = park_link.text.strip()
    else:
      park_name = columns[0].text.strip()
    visitors = columns[5].text.strip().replace(',', '')
    national_parks_data.append({"National Park": park_name, "Visitors": visitors})
  else:
    park_name_th = row.find("th")
    if park_name_th:
      park_link = park_name_th.find("a")
      if park_link:
        park_name = park_link.text.strip()
        visitors = columns[4].text.strip().replace(',', '')
        national_parks_data.append({"National Park": park_name, "Visitors": visitors})

national_parks_df = pd.DataFrame(national_parks_data)

national_parks_df["National Park"] = national_parks_df["National Park"].apply(lambda x: x + " National Park")
national_parks_df.loc[national_parks_df['National Park'] == 'Denali National Park', 'National Park'] = 'Denali National Park & Preserve'
national_parks_df.loc[national_parks_df['National Park'] == 'Gates of the Arctic National Park', 'National Park'] = 'Gates of the Arctic National Park & Preserve'
national_parks_df.loc[national_parks_df['National Park'] == 'Glacier Bay National Park', 'National Park'] = 'Glacier Bay National Park & Preserve'
national_parks_df.loc[national_parks_df['National Park'] == 'Great Sand Dunes National Park', 'National Park'] = 'Great Sand Dunes National Park & Preserve'
national_parks_df.loc[national_parks_df['National Park'] == 'Katmai National Park', 'National Park'] = 'Katmai National Park & Preserve'
national_parks_df.loc[national_parks_df['National Park'] == 'Kings Canyon National Park', 'National Park'] = 'Sequoia & Kings Canyon National Parks'
national_parks_df.loc[national_parks_df['National Park'] == 'Lake Clark National Park', 'National Park'] = 'Lake Clark National Park & Preserve'
national_parks_df.loc[national_parks_df['National Park'] == 'Redwood National Park', 'National Park'] = 'Redwood National and State Parks'
national_parks_df.loc[national_parks_df['National Park'] == 'Sequoia National Park', 'National Park'] = 'Sequoia & Kings Canyon National Parks'
national_parks_df.loc[59, 'National Park'] = 'Wrangell - St Elias National Park & Preserve'
national_parks_df.loc[national_parks_df['National Park'] == 'American Samoa National Park', 'National Park'] = 'National Park of American Samoa'
national_parks_df.loc[national_parks_df['National Park'] == 'New River Gorge National Park', 'National Park'] = 'New River Gorge National Park & Preserve'

national_parks_df

def fetch_park_data(api_url, park_codes, api_key):
  park_data = []
  for park_code in park_codes:
    url = f"{api_url}?parkCode={park_code}&api_key={api_key}"
    response = requests.get(url)
    if response.status_code == 200:
      data = response.json()
      for park in data["data"]:
        park_name = park["fullName"]
        state = park["states"]
        city = park["addresses"][0]["city"] if park["addresses"] else "N/A"
        park_zip = park["addresses"][0]["postalCode"] if park["addresses"] else "N/A"
        park_data.append({"Park Name": park_name, "State": state, "City": city, "Zip Code": park_zip})
  return park_data

api_url = "https://developer.nps.gov/api/v1/parks"
park_codes = ['NPSA', 'JEFF', 'ACAD', 'ARCH', 'BADL', 'BIBE', 'BISC', 'BLCA', 'BRCA', 'CANY', 'CARE', 'CAVE', 'CHIS', 'CONG', 'CRLA', 'CUVA', 'DEVA', 'DENA', 'DRTO', 'EVER', 'GAAR', 'GLAC', 'GLBA', 'GRCA', 'GRTE', 'GRBA', 'GRSA', 'GRSM', 'GUMO', 'HALE', 'HAVO', 'HOSP', 'INDU', 'ISRO', 'JOTR', 'KATM', 'KEFJ', 'SEKI', 'KOVA', 'LACL', 'LAVO', 'MACA', 'MEVE', 'MORA', 'NERI', 'NPSA', 'NOCA', 'OLYM', 'PEFO', 'PINN', 'REDW', 'ROMO', 'SAGU', 'SEKI', 'SHEN', 'THRO', 'VIIS', 'VOYA', 'WHSA', 'WICA', 'WRST', 'YELL', 'YOSE', 'ZION']
api_key = "c568bXQvtf8sNnNDCS9IO12FnW0nRLhbVCux0g8h"

park_data = fetch_park_data(api_url, park_codes, api_key)

api_df = pd.DataFrame(park_data)

api_df

national_parks_df['National Park'] = national_parks_df['National Park'].str.lower()
api_df['Park Name'] = api_df['Park Name'].str.lower()
api_df['City'] = api_df['City'].str.lower()
api_df['State'] = api_df['State'].str.lower()

merged_df = pd.merge(national_parks_df, api_df[['Park Name', 'City', 'State', 'Zip Code']], left_on="National Park", right_on="Park Name")

merged_df.drop(columns=["Park Name"], inplace=True)

merged_df['State'] = merged_df['State'].str.split(',').str[0]

merged_df = merged_df.drop(1)
merged_df = merged_df.drop(38)
merged_df = merged_df.drop(40)

merged_df

merged_df.to_csv('national_parks.csv', index=False)

FileLink('national_parks.csv')

# FileLink('../data/national_parks.csv') # commented out by CK for script running
