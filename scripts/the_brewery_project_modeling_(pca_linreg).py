# -*- coding: utf-8 -*-
"""The Brewery Project - Modeling (PCA/LinReg).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tLokp3b8B17W7w5xXBSsYvpYfR9xsc3d
"""

# import necessary libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale

# load and view dataset

brewery = pd.read_csv("/content/city_level.csv")
brewery

# we have identified 5 local features that could impact brewery frequency
# reduced features to 5 identified features

keep_columns = ['city_brewery_count', 'ski_resort_count', 'state_national_park_count', 'tech_city', 'major_city', 'college_town']
df = brewery[keep_columns]

# view reduced dataframe

df

# check for missing values

df.isna().sum()

# set 'city_brewery_count' as PCA target

brewery_target = df['city_brewery_count'].to_numpy()
df = pd.DataFrame(df.values, columns=df.columns)
df['city_brewery_count'] = brewery_target

# scale data

X = df.iloc[:, 1:].values
X_normal = scale(X)

# check shape of data

X_normal.shape

# implement PCA analysis

pca = PCA(n_components = 5)

principalComponents = pca.fit_transform(X_normal)

principalDf = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4',
                          'principal component 5'])

finalDf = pd.concat([principalDf, df[['city_brewery_count']]], axis = 1)

# check PC explained variance

explained_variance = pca.explained_variance_ratio_
explained_variance

# check cumulative variance

pca.explained_variance_ratio_.cumsum()

# visualize cumulative variance

cumulative_variance = np.cumsum(explained_variance)

cumulative_variance_df = pd.DataFrame({
    'Principal Component': [f'PC{i+1}' for i in range(len(cumulative_variance))],
    'Cumulative Variance': cumulative_variance
})

cumulative_variance_df.round(4)

# plot PC explained variance

plt.figure(figsize=(10, 6))
plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, align='center')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(1, len(explained_variance) + 1))
plt.title('Individual Variance Explained by Each Principal Component')
plt.grid(True)
plt.show()

# plot PCA analysis

PCA_df = finalDf[['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5']]

fig, axes = plt.subplots(2,2, figsize=(10,10))

for i, ax in enumerate(axes.flat):
  if i < len(PCA_df.columns) - 1:
    scatter = ax.scatter(PCA_df.iloc[:, i], PCA_df.iloc[:, i + 1], c=finalDf['city_brewery_count'], cmap='viridis', alpha=0.5)
    ax.set_xlabel(f'Principal Component {i + 1}')
    ax.set_ylabel(f'Principal Component {i + 2}')
    plt.colorbar(scatter, ax=ax, label='city_brewery_count')

plt.tight_layout()
plt.show()

# import libraries

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import statsmodels.api as sm
import seaborn as sns

# set 'city_brewery_count' as response variable and all other features as predictors

X = df.drop(columns=['city_brewery_count'])
y = df['city_brewery_count']


# split dataset into train and test sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1989)

# fit linear regression model using skikitlearn module

lmod = LinearRegression()
lmod.fit(X_train, y_train)
y_pred_full = lmod.predict(X_test)


# calculate and print MSPE for full model

mspe_full = ((y_test - y_pred_full) ** 2).mean()
mspe_full

# print coefficients for full model

coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': lmod.coef_})
intercept_row = pd.DataFrame({'Feature': ['Intercept'], 'Coefficient': lmod.intercept_})
coefficients = pd.concat([intercept_row, coefficients], ignore_index=True)
coefficients

# plot residuals for full model

residuals = y_test - y_pred_full
plt.scatter(y_pred_full, residuals, color='slateblue')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.axhline(y=0, color='orchid', linestyle='--')
plt.show()

# fit linear regression model using statsmodels module, print summary output

X_train = sm.add_constant(X_train)

lmod_full = sm.OLS(y_train, X_train)
lmod_full_results = lmod_full.fit()

print(lmod_full_results.summary())

# calculate MSPE for full model with constant

X_test = sm.add_constant(X_test, has_constant='add')

y_pred_full = lmod_full_results.predict(X_test)

squared_diff_full = (y_test - y_pred_full) ** 2

mspe_full = squared_diff_full.mean()
mspe_full

# implementing backward selection
# removing "state_national_park_count" feature as it has highest p-value and p-value > alpha=0.05 threshold

X_train = X_train.drop(columns=['state_national_park_count'])
X_test = X_test.drop(columns=['state_national_park_count'])

reduced_model = sm.OLS(y_train, X_train)
reduced_model_results = reduced_model.fit()

print(reduced_model_results.summary())

# calculate MSPE for reduced model

y_pred_reduced = reduced_model_results.predict(X_test)

mspe_reduced = ((y_test - y_pred_reduced) ** 2).mean()
mspe_reduced

# implementing backward selection continued
# removing "ski_resort_count" feature as it now has highest p-value and p-value > alpha=0.05 threshold

X_train = X_train.drop(columns=['ski_resort_count'])
X_test = X_test.drop(columns=['ski_resort_count'])

reduced_model = sm.OLS(y_train, X_train)
reduced_model_results = reduced_model.fit()

print(reduced_model_results.summary())

# calculate MSPE for reduced model

y_pred_reduced = reduced_model_results.predict(X_test)

mspe = ((y_test - y_pred_reduced) ** 2).mean()
mspe

# plot residuals for reduced model

residuals = y_test - y_pred_reduced
plt.scatter(y_pred_reduced, residuals, color='slateblue')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.axhline(y=0, color='orchid', linestyle='--')
plt.show()