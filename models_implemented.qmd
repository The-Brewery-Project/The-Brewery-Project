---
title: "Models Implemented"
bibliography: references.bib
csl: diabetologia.csl
---

Here we consolidate our data and results from our exploratory analyses to answer our research questions and ultimately provide for multiple facets within the brewery community.

# Consolidated Data

Using datasets featured in our exploratory analysis, we merged them together in a format conducive to begin building models.

Most columns are boolean or numeric based already, but some columns were kept in categorical format for additional exploratory analysis.

Additionally, some entries contain missing values that we've kept in effort to prevent information loss. We'll test different subsets of columns and rows to produce the best models.

The consolidation process can be found [here](https://github.com/The-Brewery-Project/The-Brewery-Project/blob/main/scripts/dataset_manager.py).

## The Starting Dataset

```{python}
import numpy as np
import pandas as pd
from itables import show
city_df = pd.read_csv('data/city_level.csv')
```

```{python}
show(city_df)
```


# Addressing Frequency

- Principal Component Analysis (PCA)
- Collinearity Check

## Frequency by Outdoor Areas

## Frequency by College Towns

## Frequency by Tech Hubs

## Frequency by Metropolitan Areas

# Defining and Classifying Brewery Hotspots

In this section, we'll create a defintion of hotspots and then create a classification model to predict how much of a hotspot a given city is (or should be).

## Defining and Preparing

### Preparation for Modeling

We will want to perform the following in preparation for modeling:

- remove `city` (unique identifiers)
- remove `state` (51 more columns don't seem necessary, especially when there is state specific data)
- remove `2021 median income` (86.4% missing values)
- remove `brewery_concentration` (a variable created from population and city brewery count)
- remove `per_capita_ranked` (a variable crated from brewery_concentration)
- remove `city_brewery_count` (basis for custom_ranked)
- remove rows where the census data is failed to be captured
- encode `region`

In defining brewery hotspots, we went with two separate methods:

- per capita (per 1,000 people) - based on population and brewery counts per city with 10 classes
- custom method - based on brewery counts per city and uses additional rules 

Model brewery hotspot criteria:

- based on counts per city
- uses a rule of any city below 3 breweries would be categorized as a 1 (low)
- bins across 6 categories

> Due to `city_brewery_count` being used in creating the hotspot criteria, it was removed for modeling.

After this process, our dataset is ready for modeling. Here are the first 10 rows of this modeling dataset:

```{python}
model_df_head = pd.read_csv('data/model_data.csv')
show(model_df_head)
```

## Classifying Brewery Hotspots

Now that we have defined a hotspot ranking system and prepared our dataset for modeling, it's time to create some classification models!

The complete classification modeling process can be found [here](https://github.com/The-Brewery-Project/The-Brewery-Project/blob/main/scripts/brewery-hotspots.py).

Our modeling process will consist of:

- Performing default algorithms, reporting metrics
- Applying scaling, performing default algorithms, reporting metrics
- Running hypertuning on the top performing default algorithm(s)

The metrics we'll be reporting on are:

- **Accuracy**: the ratio of correct predictions to all predictions
- **Precision**: the ratio of true positives to the total number of positives (a measure of exactness)
- **Recall**: the ratio of true positives to the number of total correct predictions (a measure of completeness)
- **F1-Score**: the *harmonic mean* between precision and recall (a balanced combination, or equal weight, of both precision and recall)

Using the **sklearn** library, the default classification algorithms we'll be testing are:

- **Decision Tree**: `DecisionTreeClassifier()`
- **Logistic Regression**: `LogisticRegression()`
- **K Nearest Neighbors**: `KNeighborsClassifier()`
- **Support Vector Machine**: `VC()`
- **Naive Baye**s: `GaussianNB()`
- **Linear Discriminant Analysis**: `LinearDiscriminantAnalysis()`

### Default Algorithms

After performing the default algorithms, we found that the `DecisionTreeClassifier()` performed the best across the metrics Accuracy, Precision, Recall, and F1-Score for a non-scaled data single-run. Several of the other classifiers also performed well. *Note that even though this was supposed to be strictly a default algorithm section,* `LogisticRegression()` *required a change to the* `max_iter` *parameter to even run.*

Here is a printout of the results:

```{python}
classification_round_1 = pd.read_csv('data/hotspot_round1.csv')
show(classification_round_1)
```

### Default Algorithms with Scaled Data

Next, we'll perform the same process with the default algorithms, this time using scaled data applied with `StandardScaler()`.

After performing the default algorithms, we found that the `LogisticRegression()` performed the best across the metrics Accuracy, Precision, Recall, and F1-Score for a scaled data single-run. Several of the other classifiers also performed well. *Note that even though this was supposed to be strictly a default algorithm section,* `LogisticRegression()` *required a change to the* `max_iter` *parameter to even run.*

Here is a printout of the results:

```{python}
classification_round_2 = pd.read_csv('data/hotspot_round2.csv')
show(classification_round_2)
```

### Visualizing the Default Results

**Default Algorithms with Non-Scaled Data**

![](images/hotspot_models/hotspot_round_1_a.png){fig-align="center"}

**Default Algorithms with Scaled Data**

![](images/hotspot_models/hotspot_round_1_b.png){fig-align="center"}

To summarize, `DecisionTreeClassifier()` performed the best for a non-scaled data single-run and `LogisticRegression()` performed the best for a scaled data single-run. 

Here is a visualization of the top performers from each data type:

![](images/hotspot_models/top_performing_default.png){fig-align="center"}


### Parameter Hypertuning

Using the sklearn library, we can use `GridSearchCV()` to test a multitude of different combinations for algorithms.

For `DecisionTreeClassifier()` (non-scaled data), we will test the following parameters:

- **criterion**: gini, entropy, log_loss
- **splitter**: best, random
- **max_depth**: None, 2, 4, 6, 8, 10
- **max_features**: None, sqrt, log2
- **class_weight**: None, balanced

The results across all possible combinations are as follows:

```{python}
hypertuned_df = pd.read_csv('data/hotspot_hyper_tree_results.csv')
show(hypertuned_df)
```

From this process, we deduced that the best parameters for this model are:

- **criterion**: entropy
- **splitter**: best
- **max_depth**: 6
- **max_features**: None
- **class_weight**: None

Namely, using *entropy* as the criterion and *max_depth* of 6 resulted in the best best model for the `DecisionTreeClassifier()` (non-scaled data).


For `LogisticRegression()` (scaled data), we will test the following parameters:

- **class_weight**: None, balanced
- **max_iter**: 10000
- **multi_class**: auto, multinomial
- **penalty**: None, l2, l1, balanced
- **solver**: lbfgs, saga

The results across all possible combinations are as follows:

```{python}
hypertuned_df = pd.read_csv('data/hotspot_hyper_logistic_results.csv')
show(hypertuned_df)
```

From this process, we deduced that the best parameters for this model are:

- **class_weight**: None
- **max_iter**: 10000
- **multi_class**: auto
- **penalty**: l1
- **solver**: saga

Namely, using *max_iter* of 10000, *penalty* of l1, and *solver* as saga resulted in the best best model for the `LogisticRegression()` (scaled data).

Comparing our metrics for the two best models resulted in:

```{python}
hypertuned_df = pd.read_csv('data/best_hyper_results.csv')
show(hypertuned_df)
```

Visualizing the best results:

![](images/hotspot_models/hypertuned_models.png){fig-align="center"}

Overall, `LogisticRegression()` with scaled data performs the best with scores in Accuracy, Precision, Recall, F1-Score in the mid-90s.

One final note is that the overall scores for the *best* `DecisionTreeClassifier()` are lower than the originally run model. This is due to how `GridSearchCV()` tests models, which is through cross validation. The default (which we had ran the models through) uses 5-fold cross validation. This means that the *initial data is randomly partitioned into 5 mutually exclusive subsets (folds), each of approximately equal size, and then training and testing are performed 5 times* [@datamining4th]. This helps in assuring a better model than just using a single split. Although not produced during the timeframe of this project, with how the scores for the `DecisionTreeClassifier()` changed, and how much of an increase `LogisticRegression()` received, it may be worth utilizing `GridSearchCV()` across more of the models at a later time.

### Applying the Model

> Note that the best fitting model was saved as a pickle file, and can be found [here](https://github.com/The-Brewery-Project/The-Brewery-Project/blob/main/models/hotspot_model.pkl).


# Outlier Analysis

# Where Can I Find a Brewery?

- Decision Tree

## For the Brewer

## For the Consumer
